from typing import Dict, Any, Optional, List
import threading
import openai
import os
import json
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()


class ProcessContextManager:
    """
    proc_inst_idë³„ë¡œ ì‘ì—… ë‚´ìš©ì„ íŒŒì¼(json)ë¡œë§Œ ì €ì¥/ì¡°íšŒí•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (ë©”ëª¨ë¦¬ ìºì‹œ ì—†ìŒ)
    í•˜ë‚˜ì˜ contexts.json íŒŒì¼ì— ëª¨ë“  proc_inst_idë¥¼ í‚¤ë¡œ í•˜ì—¬ ì €ì¥
    """
    _instance = None
    _lock = threading.Lock()
    _context_file = Path(__file__).parent / "contexts.json"
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            openai.api_key = os.getenv("OPENAI_API_KEY")
            self.openai_client = openai
            print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.openai_client = None

    def _load_all_contexts(self) -> Dict[str, Any]:
        """ì „ì²´ ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
        if self._context_file.exists():
            try:
                with self._context_file.open("r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return {}
    
    def _save_all_contexts(self, all_contexts: Dict[str, Any]):
        """ì „ì²´ ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥"""
        try:
            with self._context_file.open("w", encoding="utf-8") as f:
                json.dump(all_contexts, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _summarize_reports(self, reports: Dict[str, str]) -> str:
        """LLMì„ ì‚¬ìš©í•´ ë¦¬í¬íŠ¸ë“¤ì„ ìš”ì•½"""
        if not self.openai_client or not reports:
            return "ìš”ì•½ ë¶ˆê°€"
        
        print("\n\nìš”ì•½ì„ ìœ„í•œ LLMí˜¸ì¶œ ì‹œì‘\n\n")
        
        # ëª¨ë“  ë¦¬í¬íŠ¸ í•©ì¹˜ê¸°
        combined_reports = "\n\n=== ë¦¬í¬íŠ¸ êµ¬ë¶„ ===\n\n".join(reports.values())
        
        prompt = f"""ë‹¤ìŒì€ ì´ì „ ìš”ì•½ ë‚´ìš©ê³¼ ìƒˆë¡œ ì¶”ê°€ëœ ì‚°ì¶œë¬¼(í¼, ë¦¬í¬íŠ¸ ë“±)ì…ë‹ˆë‹¤. 
ì´ì „ ìš”ì•½ê³¼ ìƒˆ ì‚°ì¶œë¬¼ì„ ë³‘í•©í•˜ì—¬, ì•„ë˜ í˜•ì‹ì— ë§ëŠ” í•˜ë‚˜ì˜ í†µí•© ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”. 

ë°˜ë“œì‹œ ì§€ì¼œì•¼í•˜ëŠ” ì‚¬í•­ë“¤ : 
    1. ì•„ë˜ í˜•ì‹ì€ 'ë³´ê³ ì„œ'ê°€ ì•„ë‹ˆë¼, ë‹¨ìˆœíˆ ì •ë³´ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ìš”ì•½ ì–‘ì‹ì¼ ë¿ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ì „ ìš”ì•½ê³¼ ìƒˆ ì‚°ì¶œë¬¼ì˜ ëª¨ë“  í•µì‹¬ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ë°˜ì˜í•˜ì—¬, ë³‘í•©ëœ í•˜ë‚˜ì˜ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
    2. ì „ì²´ ë‚´ìš©ì€ ë°˜ë“œì‹œ 2000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš” (ì•½ A4ìš©ì§€ í•œì¥ ì •ë„)
    3. ë‚´ìš©ì„ ëˆ„ì í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ì‹ ê·œ ì‚°ì¶œë¬¼ê³¼ í˜„ì¬ ìš”ì•½ì„ í•©ì³ì„œ ìƒˆë¡œìš´ 2000ì ì´ë‚´ì˜ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.

    
ë¦¬í¬íŠ¸ ë‚´ìš©:
{combined_reports}

===== ìš”ì•½ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”) =====

ğŸ“‹ ë³´ê³ ì„œ ì œëª©: [ë¦¬í¬íŠ¸ì—ì„œ ì •í™•íˆ ì¶”ì¶œí•œ ì œëª© ì—†ìœ¼ë©´, ë¬¸ë§¥ìƒ íë¦„ì„ ë¶„ì„í•˜ì—¬ ì œëª©ì„ ì •ì˜]

ğŸ“Œ ëª©ì  : [ì‚¬ìš©ì ìš”ì²­ ë° ë¬¸ë§¥ìƒ íë¦„ì„ ë¶„ì„í•˜ì—¬ ëª©ì ì„ ì •ì˜]
ğŸ“Œ ìš”êµ¬ì‚¬í•­ : [ì‚¬ìš©ì ìš”ì²­ ë° ë¬¸ë§¥ìƒ íë¦„ì„ ë¶„ì„í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ì •ì˜]
ğŸ“Œ í”¼ë“œë°± : [ì‚¬ìš©ì ìš”ì²­ ë° ë¬¸ë§¥ìƒ íë¦„ì„ ë¶„ì„í•˜ì—¬ í”¼ë“œë°±ì„ ì •ì˜]
ğŸ“Œ ì´ìŠˆ : [ì‚¬ìš©ì ìš”ì²­ ë° ë¬¸ë§¥ìƒ íë¦„ì„ ë¶„ì„í•˜ì—¬ ì´ìŠˆë¥¼ ì •ì˜]

ğŸ‘¤ ì‘ì„± ì •ë³´:
- ì‘ì„±ì: [ì‘ì„±ìëª…]
- ì†Œì†ë¶€ì„œ: [ë¶€ì„œëª…]

ğŸ¯ ëª©ì°¨ë³„ í•µì‹¬ ìš”ì•½:

1ï¸âƒ£ [ëª©ì°¨1 ì œëª©]:
   â€¢ í•µì‹¬ë‚´ìš© 1: [ì¤‘ìš” í¬ì¸íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ]
   â€¢ í•µì‹¬ë‚´ìš© 2: [ì£¼ìš” ë°ì´í„°ë‚˜ ê²°ê³¼ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ]
   â€¢ í•µì‹¬ë‚´ìš© 3: [ê²°ë¡ ì´ë‚˜ ì‹œì‚¬ì ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ]

2ï¸âƒ£ [ëª©ì°¨2 ì œëª©]:
   â€¢ í•µì‹¬ë‚´ìš© 1: [ì¤‘ìš” í¬ì¸íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ]
   â€¢ í•µì‹¬ë‚´ìš© 2: [ì£¼ìš” ë°ì´í„°ë‚˜ ê²°ê³¼ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ]
   â€¢ í•µì‹¬ë‚´ìš© 3: [ê²°ë¡ ì´ë‚˜ ì‹œì‚¬ì ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ]

3ï¸âƒ£ [ëª©ì°¨3 ì œëª©]:
   â€¢ í•µì‹¬ë‚´ìš© 1: [ì¤‘ìš” í¬ì¸íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ]
   â€¢ í•µì‹¬ë‚´ìš© 2: [ì£¼ìš” ë°ì´í„°ë‚˜ ê²°ê³¼ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ]
   â€¢ í•µì‹¬ë‚´ìš© 3: [ê²°ë¡ ì´ë‚˜ ì‹œì‚¬ì ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ]

[ê³„ì†í•´ì„œ ëª¨ë“  ëª©ì°¨ì— ëŒ€í•´ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ...]

ğŸ¯ ì „ì²´ ìš”ì•½:
- ì£¼ìš” ëª©ì : [í•µì‹¬ ëª©ì ]
- í•µì‹¬ ê²°ê³¼: [ê°€ì¥ ì¤‘ìš”í•œ ê²°ê³¼ë‚˜ ë°œê²¬ì‚¬í•­]
- í–¥í›„ ê³„íš: [ì œì•ˆì‚¬í•­ì´ë‚˜ í›„ì† ì¡°ì¹˜]

===== ì‘ì„± ì§€ì¹¨ =====
!!ì¤‘ìš”!! ì „ì²´ ë‚´ìš©ì€ 2000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. 
1. ëª©ì°¨ëŠ” ë¦¬í¬íŠ¸ì—ì„œ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ ëˆ„ë½ ì—†ì´ ëª¨ë‘ í¬í•¨
2. ê° ëª©ì°¨ë³„ë¡œ ë°˜ë“œì‹œ 3ê°œì˜ í•µì‹¬ë‚´ìš©ì„ ì¶”ì¶œ (ë¶€ì¡±í•˜ë©´ ê´€ë ¨ ë‚´ìš©ìœ¼ë¡œ ë³´ì™„)
3. ìˆ«ì, ë°ì´í„°, êµ¬ì²´ì  ì‚¬ì‹¤ì„ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨
4. í•œ ë¬¸ì¥ì€ ìµœëŒ€ 50ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
5. ë©”íƒ€ë°ì´í„°(ì‘ì„±ì, ë¶€ì„œ ë“±)ëŠ” ë°˜ë“œì‹œ ì°¾ì•„ì„œ í¬í•¨ (ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œ)
6. ì´ëª¨ì§€ì™€ êµ¬ì¡°í™”ëœ í˜•ì‹ì„ ì •í™•íˆ ìœ ì§€
7. ì „ë¬¸ìš©ì–´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª… ì¶”ê°€
8. ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ë‚´ìš© ë°°ì¹˜"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "system", "content": """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                    
ì£¼ìš” ì—­í• :
- ë³µì¡í•œ ì‚°ì¶œë¬¼(ë³´ê³ ì„œ, í¼ ë“±)ì„ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ìš”ì•½
- ëª©ì°¨ë³„ í•µì‹¬ ë‚´ìš©ì„ ë¹ ì§ì—†ì´ ì¶”ì¶œ
- ë©”íƒ€ë°ì´í„°ì™€ ì¤‘ìš” ë°ì´í„°ë¥¼ ì •í™•íˆ íŒŒì•…
- ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œì˜ í•µì‹¬ ê°€ì¹˜ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ê°„ê²°í•˜ê²Œ ì •ë¦¬

ì‘ì—… ì›ì¹™:
1. ì •í™•ì„±: ì›ë¬¸ì˜ ë‚´ìš©ì„ ì™œê³¡í•˜ì§€ ì•Šê³  ì •í™•íˆ ìš”ì•½
2. ì™„ì „ì„±: ëª¨ë“  ëª©ì°¨ì™€ ì¤‘ìš” ì •ë³´ë¥¼ ëˆ„ë½ ì—†ì´ í¬í•¨
3. êµ¬ì¡°í™”: ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì½ê¸° ì‰½ê²Œ ì •ë¦¬
4. ê°„ê²°ì„±: í•µì‹¬ë§Œ ì¶”ì¶œí•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì „ë‹¬
5. ì‹¤ìš©ì„±: í›„ì† ì‘ì—…ì— í™œìš©í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ê°€ê³µ"""},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,
                temperature=0.1
            )
            
            summary = response.choices[0].message.content.strip()
            print(f"âœ… ë¦¬í¬íŠ¸ ìš”ì•½ ì™„ë£Œ: {len(summary)}ì")
            return summary
            
        except Exception as e:
            print(f"âŒ ë¦¬í¬íŠ¸ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
    
    def save_context(self, proc_inst_id: str, activity_name: str, content: Any):
        """
        ì»¨í…ìŠ¤íŠ¸ì— ë°ì´í„° ì €ì¥ (í•˜ë‚˜ì˜ íŒŒì¼ì—ì„œ proc_inst_idë³„ë¡œ ê´€ë¦¬)
        Args:
            proc_inst_id: í”„ë¡œì„¸ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ID
            activity_name: ì‚°ì¶œë¬¼/í¼ì˜ ì•¡í‹°ë¹„í‹° ì´ë¦„(êµ¬ë¶„ì, ë¡œê·¸ìš©)
            content: ì €ì¥í•  ë‚´ìš© (dict, str ë“±)
        """
        print(f"ğŸ’¾ [SAVE_CONTEXT] {proc_inst_id} / {activity_name}")
        if not proc_inst_id or not activity_name:
            return
        
        with self._lock:  # ë™ì‹œ ì ‘ê·¼ ë°©ì§€
            # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
            all_contexts = self._load_all_contexts()
            # í˜„ì¬ proc_inst_idì˜ ê¸°ì¡´ ë°ì´í„°
            current_data = all_contexts.get(proc_inst_id, {})
            # ê¸°ì¡´ summary
            prev_summary = current_data.get("summary", None)
            # report ì‚°ì¶œë¬¼ì´ ìˆëŠ”ì§€ í™•ì¸
            has_report = False
            if isinstance(content, dict):
                if "report" in content or "reports" in content:
                    reports_data = content.get("reports", content.get("report"))
                    if reports_data:
                        has_report = True
            if "report" in activity_name.lower():
                has_report = True
            
            # ìƒˆ contentì— reportê°€ ìˆìœ¼ë©´ prev_summary ìœ ë¬´ì™€ ê´€ê³„ì—†ì´ ìš”ì•½
            if has_report:
                print(f"ğŸ¤– [LLM_SUMMARY] ìš”ì•½ ì‹œì‘")
                # ìš”ì•½ í”„ë¡¬í”„íŠ¸ êµ¬ì„±: ì´ì „ summary + ìƒˆ content
                merged_for_summary = {}
                if prev_summary:
                    merged_for_summary["ì´ì „ ìš”ì•½"] = prev_summary
                if isinstance(content, (dict, list)):
                    merged_for_summary[activity_name] = json.dumps(content, ensure_ascii=False, indent=2)
                else:
                    merged_for_summary[activity_name] = str(content)
                summary = self._summarize_reports(merged_for_summary)
            else:
                # contentë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ì €ì¥
                if isinstance(content, (dict, list)):
                    summary = json.dumps(content, ensure_ascii=False, indent=2)
                else:
                    summary = str(content)
            
            # í˜„ì¬ proc_inst_id ë°ì´í„° ì—…ë°ì´íŠ¸
            all_contexts[proc_inst_id] = {"summary": summary}
            # ì „ì²´ íŒŒì¼ ì €ì¥
            self._save_all_contexts(all_contexts)
    
    def get_context(self, proc_inst_id: str) -> Dict[str, Any]:
        """
        ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (í•˜ë‚˜ì˜ íŒŒì¼ì—ì„œ íŠ¹ì • proc_inst_id ì¡°íšŒ)
        
        Args:
            proc_inst_id: í”„ë¡œì„¸ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ID
            
        Returns:
            í•´ë‹¹ proc_inst_idì˜ ë°ì´í„°
        """
        if not proc_inst_id:
            return {}
        
        with self._lock:  # ë™ì‹œ ì ‘ê·¼ ë°©ì§€
            all_contexts = self._load_all_contexts()
            data = all_contexts.get(proc_inst_id, {})
            print(f"ğŸ“– [GET_CONTEXT] {proc_inst_id}")
            return data

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
context_manager = ProcessContextManager() 